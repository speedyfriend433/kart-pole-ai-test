<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>Enhanced Cart-Pole AI Balancer – Multiple Pole Options</title>
    <!-- TensorFlow.js CDN -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
    <style>
      body, html {
        margin: 0;
        padding: 0;
        overflow: hidden;
        font-family: sans-serif;
      }
      canvas {
        display: block;
      }
      #controlPanel {
        position: absolute;
        top: 10px;
        left: 10px;
        background: rgba(255, 255, 255, 0.9);
        padding: 10px;
        border-radius: 5px;
        z-index: 100;
      }
      #controlPanel label, #controlPanel button, #controlPanel select {
        font-size: 13px;
      }
      #metrics {
        margin-top: 10px;
        font-size: 12px;
      }
    </style>
  </head>
  <body>
    <!-- WebGL canvas -->
    <canvas id="glcanvas" width="800" height="600"></canvas>
    <!-- Control Panel -->
    <div id="controlPanel">
      <!-- Simulation control buttons -->
      <button id="toggleSim">Pause Simulation</button>
      <button id="resetSim">Reset Simulation</button>
      <button id="toggleRecord">Start Recording</button>
      <br><br>
      <!-- Parameter sliders -->
      <label>Force Magnitude: <span id="forceDisplay">10.0</span></label><br>
      <input type="range" id="forceSlider" min="1" max="20" step="0.5" value="10">
      <br>
      <label>Gravity: <span id="gravityDisplay">9.8</span></label><br>
      <input type="range" id="gravitySlider" min="0" max="20" step="0.1" value="9.8">
      <br>
      <label>Simulation Speed: <span id="speedDisplay">1.0</span></label><br>
      <input type="range" id="speedSlider" min="0.5" max="2" step="0.1" value="1.0">
      <br><br>
      <!-- New: Dropdown to select simulation configuration -->
      <label>Pole Configuration: </label>
      <select id="poleConfig">
        <option value="1-pole">1 Pole</option>
        <option value="2-pole">2 Poles</option>
      </select>
      
      <!-- Metrics overlay -->
      <div id="metrics"></div>
    </div>
    
    <script>
      /******************************************************
       * Global Simulation and RL Variables
       ******************************************************/
      // Adjustable physics parameters (via sliders)
      let gravity = 9.8;
      const massCart = 1.0;
      const massPole = 0.1;  
      const totalMassSingle = massCart + massPole;
      // For the double pole, we treat the poles as two units of massPole.
      const totalMassDouble = massCart + 2 * massPole;
      
      let poleLength = 0.5;  // half-length (meters)
      let forceMag = 10.0;
      const baseTau = 0.02;  // base simulation time step (seconds)
      let simulationSpeed = 1.0;  // multiplier
      
      // Common cart variables
      let x = 0, x_dot = 0;
      
      // Single-pole variables
      let theta = 0.05, theta_dot = 0;
      
      // For double-pole mode, we use two angles:
      let theta1 = 0.05, theta1_dot = 0;
      let theta2 = 0.05, theta2_dot = 0;
      
      // Scale and offset for rendering (pixels)
      const scaleFactor = 100;
      const offsetY = 100;
      
      // Trajectory for the tip (for drawing history)
      let trajectory = [];
      
      // Reinforcement learning and training storage
      let states = [];
      let actions = [];
      let rewards = [];
      let episodeReward = 0, episodeCount = 0;
      
      // Simulation Mode: "1-pole" or "2-pole" (default 1-pole)
      let simulationType = "1-pole";
      
      // TensorFlow.js model – will be (re)created based on simulationType.
      let model, optimizer;
      
      // Initialize the neural network model based on simulationType:
      function setupModel() {
        if (model) { model.dispose(); }  // Dispose of any existing model
        if (simulationType === "1-pole") {
          model = tf.sequential();
          model.add(tf.layers.dense({ inputShape: [4], units: 24, activation: 'relu' }));
          model.add(tf.layers.dense({ units: 24, activation: 'relu' }));
          model.add(tf.layers.dense({ units: 2, activation: 'softmax' }));
        } else if (simulationType === "2-pole") {
          model = tf.sequential();
          model.add(tf.layers.dense({ inputShape: [6], units: 30, activation: 'relu' }));
          model.add(tf.layers.dense({ units: 30, activation: 'relu' }));
          model.add(tf.layers.dense({ units: 2, activation: 'softmax' }));
        }
        optimizer = tf.train.adam(0.01);
      }
      
      setupModel();
      
      // Choose an action based on current state. For single pole, state has 4 elements;
      // for double, 6 elements.
      // Global variables for epsilon-greedy exploration.
let epsilon = 1.0;       // Start with full exploration.
const epsilonDecay = 0.995;  // Decay factor after each episode.
const minEpsilon = 0.1;      // Minimum value for epsilon.

// Modified chooseAction function using epsilon-greedy.
function chooseAction(state) {
  // With probability epsilon, choose a random action.
  if (Math.random() < epsilon) {
    return Math.random() < 0.5 ? -1 : 1;
  } else {
    // Otherwise, get an action from the policy network.
    return tf.tidy(() => {
      const logits = model.predict(tf.tensor2d([state]));
      const probs = logits.dataSync(); // e.g. [probLeft, probRight]
      // Sample action based on predicted probabilities.
      return (Math.random() < probs[0]) ? -1 : 1;
    });
  }
}
      
      /******************************************************
       * Simulation Step Functions
       ******************************************************/
      // Single-pole simulation using cart-pole dynamics.
      function simulateSingleStep(action, dt) {
        const force = action * forceMag;
        const costheta = Math.cos(theta);
        const sintheta = Math.sin(theta);
        const temp = (force + massPole * poleLength * theta_dot * theta_dot * sintheta) / totalMassSingle;
        const theta_acc = (gravity * sintheta - costheta * temp) /
                          (poleLength * (4.0/3.0 - (massPole * costheta * costheta) / totalMassSingle));
        const x_acc = temp - (massPole * poleLength * theta_acc * costheta) / totalMassSingle;
        x += dt * x_dot;
        x_dot += dt * x_acc;
        theta += dt * theta_dot;
        theta_dot += dt * theta_acc;
      }
      
      // Double-pole simulation (simplified/demonstrative).
      // The first pole (theta1) affects the cart; the second pole (theta2) is attached to the tip of the first.
      function simulateDoubleStep(action, dt) {
        const force = action * forceMag;
        // Effective mass for the first pole includes both poles:
        let m_eff = 2 * massPole;
        let costheta1 = Math.cos(theta1);
        let sintheta1 = Math.sin(theta1);
        let temp = (force + m_eff * poleLength * theta1_dot * theta1_dot * sintheta1) / (massCart + m_eff);
        let theta1_acc = (gravity * sintheta1 - costheta1 * temp) /
                         (poleLength * (4.0/3.0 - (m_eff * costheta1 * costheta1) / (massCart + m_eff)));
        let x_acc = temp - (m_eff * poleLength * theta1_acc * costheta1) / (massCart + m_eff);
        // A simplified dynamic for the second pole: it acts as a pendulum attached at the tip of the first.
        // (Note: This is an approximation solely for demonstration.)
        let theta2_acc = (gravity * Math.sin(theta2) - 0.2 * theta2_dot);
        
        x += dt * x_dot;
        x_dot += dt * x_acc;
        theta1 += dt * theta1_dot;
        theta1_dot += dt * theta1_acc;
        theta2 += dt * theta2_dot;
        theta2_dot += dt * theta2_acc;
      }
      
      /******************************************************
       * Training Function (Policy Gradient)
       ******************************************************/
      // First, create your Actor and Critic networks.
// You may adjust the architecture as needed.
let actorModel, criticModel;  // global declaration

function setupActorCriticModels() {
  // Define stateDim based on the simulation type.
  const numActions = 2; // Global constant for the two possible actions: left and right
  const stateDim = simulationType === "1-pole" ? 4 : 6; // Using the global numActions now.

  // Dispose previous models if any.
  if (actorModel) actorModel.dispose(); 
  if (criticModel) criticModel.dispose();

  // Global simulation variables
// let simulationType = "1-pole";
// For both 1-pole and 2-pole cases, we assume 2 possible actions: left (-1) and right (+1)

 // For left and right actions
  // Create (or recreate) the actor network.
  // Create the actor network. 
  actorModel = tf.sequential(); 
  actorModel.add(tf.layers.dense({ inputShape: [stateDim], units: 24, activation: 'relu' })); 
  actorModel.add(tf.layers.dense({ units: 24, activation: 'relu' })); 
  actorModel.add(tf.layers.dense({ units: numActions, activation: 'softmax' }));

// Create the critic network. 
  criticModel = tf.sequential(); 
  criticModel.add(tf.layers.dense({ inputShape: [stateDim], units: 24, activation: 'relu' })); 
  criticModel.add(tf.layers.dense({ units: 24, activation: 'relu' })); 
  criticModel.add(tf.layers.dense({ units: 1, activation: 'linear' }));

optimizer = tf.train.adam(0.01); }



// Call this function at initialization or if you change simulation type.
setupActorCriticModels();

// Now, the sophisticated training function using actor–critic.
// This function computes the discounted returns, advantage estimates,
// and then uses them to compute combined actor and critic losses (with an added entropy bonus).
function trainActorCriticModel() {
  // Compute discounted returns from the rewards array.
  const discountedReturns = [];
  let sum = 0;
  const gamma = 0.99;
  for (let i = rewards.length - 1; i >= 0; i--) {
    sum = rewards[i] + gamma * sum;
    discountedReturns[i] = sum;
  }
  
  // Convert collected data to tensors.
  const statesTensor = tf.tensor2d(states);         // shape: [numSteps, stateDim]
  const actionsTensor = tf.tensor1d(actions, 'int32'); // shape: [numSteps]
  const returnsTensor = tf.tensor1d(discountedReturns); // shape: [numSteps]
  
  // Use the optimizer to update both actor and critic.
  optimizer.minimize(() => {
    // Actor forward pass: predict action probabilities.
    const logits = actorModel.predict(statesTensor);
    const actionProbs = tf.softmax(logits);
    
    // Critic forward pass: predict state values.
    // Reshape the critic output to a 1D tensor.
    const values = criticModel.predict(statesTensor).reshape([statesTensor.shape[0]]);
    
    // Compute advantage: advantage = (discounted return) - (estimated value)
    // (Optionally, you could also normalize the advantage.)
    const advantages = returnsTensor.sub(values);
    
    // Compute the actor loss.
    // 1. Create one-hot encodings for the actions taken.
    const oneHotActions = tf.oneHot(actionsTensor, numActions);
    // 2. Compute log probabilities for each action.
    const logProbs = tf.log(tf.clipByValue(actionProbs, 1e-7, 1));
    // 3. For each step, select the log probability corresponding to the taken action.
    const selectedLogProbs = tf.mul(oneHotActions, logProbs).sum(1);
    // 4. The policy loss is the negative of the (advantage-weighted) log probabilities.
    const actorLoss = selectedLogProbs.mul(advantages).neg().mean();
    
    // Compute the entropy of the policy distribution.
    // Entropy = -sum(p * log(p)), averaged over examples.
    const entropy = tf.mul(actionProbs, logProbs).sum(1).neg().mean();
    // A small factor for the entropy bonus.
    const entBonusWeight = 0.01;
    const entropyBonus = entropy.mul(entBonusWeight);
    
    // Critic loss: mean-squared error between predicted values and discounted returns.
    const criticLoss = advantages.square().mean();
    
    // Combine the losses.
    // We add actorLoss and criticLoss, and subtract the entropy bonus so as to encourage exploration.
    const totalLoss = actorLoss.add(criticLoss).sub(entropyBonus);
    return totalLoss;
  }, 
  // Update all trainable variables from both models.
  actorModel.trainableWeights.concat(criticModel.trainableWeights));
  
  // Clean-up (dispose intermediate tensors).
  statesTensor.dispose();
  actionsTensor.dispose();
  returnsTensor.dispose();
}



      
      /******************************************************
       * WebGL Rendering Setup and Functions
       ******************************************************/
      let canvas, gl, program, positionBuffer;
      let resolutionUniformLocation, colorUniformLocation, positionAttribLocation;
      
      function initGL() {
        canvas = document.getElementById("glcanvas");
        gl = canvas.getContext("webgl");
        if (!gl) {
          alert("WebGL not supported");
          return;
        }
        const vsSource = `
          attribute vec2 a_position;
          uniform vec2 u_resolution;
          void main() {
            vec2 zeroToOne = a_position / u_resolution;
            vec2 zeroToTwo = zeroToOne * 2.0;
            vec2 clipSpace = zeroToTwo - 1.0;
            gl_Position = vec4(clipSpace * vec2(1, -1), 0, 1);
          }
        `;
        const fsSource = `
          precision mediump float;
          uniform vec4 u_color;
          void main() {
            gl_FragColor = u_color;
          }
        `;
        const vertexShader = createShader(gl, gl.VERTEX_SHADER, vsSource);
        const fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fsSource);
        program = createProgram(gl, vertexShader, fragmentShader);
        positionAttribLocation = gl.getAttribLocation(program, "a_position");
        resolutionUniformLocation = gl.getUniformLocation(program, "u_resolution");
        colorUniformLocation = gl.getUniformLocation(program, "u_color");
        positionBuffer = gl.createBuffer();
      }
      
      function createShader(gl, type, source) {
        const shader = gl.createShader(type);
        gl.shaderSource(shader, source);
        gl.compileShader(shader);
        if (gl.getShaderParameter(shader, gl.COMPILE_STATUS))
          return shader;
        console.log(gl.getShaderInfoLog(shader));
        gl.deleteShader(shader);
      }
      
      function createProgram(gl, vertexShader, fragmentShader) {
        const program = gl.createProgram();
        gl.attachShader(program, vertexShader);
        gl.attachShader(program, fragmentShader);
        gl.linkProgram(program);
        if (gl.getProgramParameter(program, gl.LINK_STATUS))
          return program;
        console.log(gl.getProgramInfoLog(program));
        gl.deleteProgram(program);
      }
      
      function setRectangle(x, y, width, height) {
        const x1 = x, y1 = y;
        const x2 = x + width, y2 = y + height;
        return new Float32Array([ x1, y1, x2, y1, x1, y2, x1, y2, x2, y1, x2, y2 ]);
      }
      
      function drawShape(vertices, color, mode) {
        gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
        gl.bufferData(gl.ARRAY_BUFFER, vertices, gl.STATIC_DRAW);
        gl.enableVertexAttribArray(positionAttribLocation);
        gl.vertexAttribPointer(positionAttribLocation, 2, gl.FLOAT, false, 0, 0);
        gl.uniform4f(colorUniformLocation, color[0], color[1], color[2], color[3]);
        gl.drawArrays(mode, 0, vertices.length / 2);
      }
      
      function drawLine(points, color, mode = gl.LINE_STRIP) {
        const vertices = new Float32Array(points);
        drawShape(vertices, color, mode);
      }
      
      function drawRect(x, y, width, height, color) {
        const vertices = setRectangle(x, y, width, height);
        drawShape(vertices, color, gl.TRIANGLES);
      }
      
      /******************************************************
       * Rendering the Scene
       ******************************************************/
      function renderScene() {
        gl.viewport(0, 0, canvas.width, canvas.height);
        gl.clearColor(0.9, 0.9, 0.9, 1);
        gl.clear(gl.COLOR_BUFFER_BIT);
        gl.useProgram(program);
        gl.uniform2f(resolutionUniformLocation, canvas.width, canvas.height);
        // Draw the ground.
        const groundY = canvas.height / 2 + offsetY;
        drawLine([0, groundY, canvas.width, groundY], [0.4, 0.4, 0.4, 1], gl.LINES);
        
        // Determine cart position.
        const cartX = canvas.width / 2 + (x * scaleFactor);
        const cartY = canvas.height / 2 + offsetY;
        // Draw the cart.
        drawRect(cartX - 25, cartY - 10, 50, 20, [0, 0, 0, 1]);
        const L = poleLength * scaleFactor;
        
        if (simulationType === "1-pole") {
          // Single-Pole: compute tip using theta.
          const tipX = cartX + Math.sin(theta) * L;
          const tipY = cartY - Math.cos(theta) * L;
          // Draw pole.
          drawLine([cartX, cartY, tipX, tipY], [1, 0, 0, 1], gl.LINES);
          // Update trajectory.
          trajectory.push({ x: tipX, y: tipY });
        } else if (simulationType === "2-pole") {
          // Double-Pole: first pole.
          const tip1X = cartX + Math.sin(theta1) * L;
          const tip1Y = cartY - Math.cos(theta1) * L;
          drawLine([cartX, cartY, tip1X, tip1Y], [1, 0, 0, 1], gl.LINES);
          // Second pole: relative to first.
          const tip2X = tip1X + Math.sin(theta1 + theta2) * L;
          const tip2Y = tip1Y - Math.cos(theta1 + theta2) * L;
          drawLine([tip1X, tip1Y, tip2X, tip2Y], [0, 0.5, 0, 1], gl.LINES);
          // Update trajectory to track the tip of the second pole.
          trajectory.push({ x: tip2X, y: tip2Y });
        }
        
        // Limit stored trajectory points.
        if (trajectory.length > 500) trajectory.shift();
        // Draw trajectory.
        if (trajectory.length > 1) {
          let pts = [];
          for (let pt of trajectory) { pts.push(pt.x, pt.y); }
          drawLine(pts, [0, 0, 1, 0.6], gl.LINE_STRIP);
        }
      }
      
      /******************************************************
       * Failure and Reset Functions
       ******************************************************/
      function checkFailure() {
        const outOfBounds = (Math.abs(x * scaleFactor) > canvas.width / 2);
        if (simulationType === "1-pole") {
          return (Math.abs(theta) > Math.PI / 2 || outOfBounds);
        } else if (simulationType === "2-pole") {
          // For double pole, declare failure if either pole deviates too far.
          return (Math.abs(theta1) > Math.PI / 2 ||
                  Math.abs(theta1 + theta2) > Math.PI / 2 ||
                  outOfBounds);
        }
      }
      
      function resetSimulation() {
        x = 0;
        x_dot = 0;
        if (simulationType === "1-pole") {
          theta = 0.05;
          theta_dot = 0;
        } else if (simulationType === "2-pole") {
          theta1 = 0.05;
          theta1_dot = 0;
          theta2 = 0.05;
          theta2_dot = 0;
        }
        trajectory = [];
      }
      
      /******************************************************
       * UI and Recording Setup
       ******************************************************/
      let simRunning = true;
      let mediaRecorder, recordedChunks = [];
      let recording = false;
      
      const toggleSimBtn = document.getElementById("toggleSim");
      const resetSimBtn = document.getElementById("resetSim");
      const toggleRecordBtn = document.getElementById("toggleRecord");
      const forceSlider = document.getElementById("forceSlider");
      const forceDisplay = document.getElementById("forceDisplay");
      const gravitySlider = document.getElementById("gravitySlider");
      const gravityDisplay = document.getElementById("gravityDisplay");
      const speedSlider = document.getElementById("speedSlider");
      const speedDisplay = document.getElementById("speedDisplay");
      const metricsDiv = document.getElementById("metrics");
      const poleConfigSelect = document.getElementById("poleConfig");
      
      toggleSimBtn.addEventListener("click", () => {
        simRunning = !simRunning;
        toggleSimBtn.textContent = simRunning ? "Pause Simulation" : "Resume Simulation";
      });
      
      resetSimBtn.addEventListener("click", resetSimulation);
      
      forceSlider.addEventListener("input", () => {
        forceMag = parseFloat(forceSlider.value);
        forceDisplay.textContent = forceMag.toFixed(1);
      });
      
      gravitySlider.addEventListener("input", () => {
        gravity = parseFloat(gravitySlider.value);
        gravityDisplay.textContent = gravity.toFixed(1);
      });
      
      speedSlider.addEventListener("input", () => {
        simulationSpeed = parseFloat(speedSlider.value);
        speedDisplay.textContent = simulationSpeed.toFixed(1);
      });
      
      poleConfigSelect.addEventListener("change", () => {
        simulationType = poleConfigSelect.value;
        resetSimulation();
        states = [];
        actions = [];
        rewards = [];
        episodeReward = 0;
        setupModel();
      });
      
      toggleRecordBtn.addEventListener("click", () => {
        if (!recording) {
          startRecording();
          toggleRecordBtn.textContent = "Stop Recording";
        } else {
          stopRecording();
          toggleRecordBtn.textContent = "Start Recording";
        }
        recording = !recording;
      });
      
      function startRecording() {
        let stream = canvas.captureStream(30);  // 30 FPS
        mediaRecorder = new MediaRecorder(stream, { mimeType: "video/webm" });
        mediaRecorder.ondataavailable = e => { if (e.data.size > 0) recordedChunks.push(e.data); };
        mediaRecorder.onstop = saveRecording;
        mediaRecorder.start();
      }
      
      function stopRecording() { mediaRecorder.stop(); }
      
      function saveRecording() {
        let blob = new Blob(recordedChunks, { type: "video/webm" });
        recordedChunks = [];
        let url = URL.createObjectURL(blob);
        let a = document.createElement("a");
        a.style.display = "none";
        a.href = url;
        a.download = "simulation.webm";
        document.body.appendChild(a);
        a.click();
        window.URL.revokeObjectURL(url);
      }
      
      // Update the metrics overlay.
      function updateMetrics() {
        if (simulationType === "1-pole") {
          metricsDiv.innerHTML = `
            Episode: ${episodeCount}<br>
            Episode Reward: ${episodeReward}<br>
            x: ${x.toFixed(2)}, x_dot: ${x_dot.toFixed(2)}<br>
            theta: ${theta.toFixed(2)}, theta_dot: ${theta_dot.toFixed(2)}
          `;
        } else if (simulationType === "2-pole") {
          metricsDiv.innerHTML = `
            Episode: ${episodeCount}<br>
            Episode Reward: ${episodeReward}<br>
            x: ${x.toFixed(2)}, x_dot: ${x_dot.toFixed(2)}<br>
            theta1: ${theta1.toFixed(2)}, theta1_dot: ${theta1_dot.toFixed(2)}<br>
            theta2: ${theta2.toFixed(2)}, theta2_dot: ${theta2_dot.toFixed(2)}
          `;
        }
      }
      
      /******************************************************
       * Main Animation Loop
       ******************************************************/
      function step() {
        if (simRunning) {
          const dt = baseTau * simulationSpeed;
          let currentState;
          let action;
          if (simulationType === "1-pole") {
            currentState = [x, x_dot, theta, theta_dot];
            action = chooseAction(currentState);
            simulateSingleStep(action, dt);
            // Store RL data.
            states.push(currentState);
            actions.push(action === -1 ? 0 : 1);
            rewards.push(Math.abs(theta) < Math.PI / 6 ? 1 : 0);
          } else if (simulationType === "2-pole") {
            currentState = [x, x_dot, theta1, theta1_dot, theta2, theta2_dot];
            action = chooseAction(currentState);
            simulateDoubleStep(action, dt);
            states.push(currentState);
            actions.push(action === -1 ? 0 : 1);
            // Reward only if both poles are near vertical.
            rewards.push((Math.abs(theta1) < Math.PI / 6 && Math.abs(theta1 + theta2) < Math.PI / 6) ? 1 : 0);
          }
          episodeReward += rewards[rewards.length - 1];
          
          if (checkFailure()) {
  episodeCount++;
  console.log("Episode:", episodeCount, "Total reward:", episodeReward);
  // Replace trainModel() with trainActorCriticModel()
  trainActorCriticModel();
  resetSimulation();
  states = [];
  actions = [];
  rewards = [];
  episodeReward = 0;
}

        }
        renderScene();
        updateMetrics();
        requestAnimationFrame(step);
      }
      
      /******************************************************
       * Initialization and Start
       ******************************************************/
      initGL();
      resetSimulation();
      requestAnimationFrame(step);
    </script>
  </body>
</html>
